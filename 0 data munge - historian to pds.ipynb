{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "todo:\n",
    "still need yaml, need to know where pd pickle is across files\n",
    "get rid of all os reads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@\n",
      "        # platform.node()='MH507192' \n",
      "        # datetime.now()=datetime.datetime(2020, 10, 20, 7, 49, 12, 608140) \n",
      "        # sys.version='3.8.3 (default, Jul  2 2020, 17:30:36) [MSC v.1916 64 bit (AMD64)]' \n",
      "        \n"
     ]
    }
   ],
   "source": [
    "%run C:/__PYP/__COMMON/ugenejupyterlib/firstcell.ipynb\n",
    "%run C:/__PYP/__COMMON/ugenejupyterlib/datacell.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "V=\\\n",
    "\"\"\"\n",
    "Foos\n",
    "\"\"\"\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import re, os\n",
    "\n",
    "def foo_initMeta():\n",
    "\n",
    "    with open(\"0_init.yaml\", 'r') as stream:\n",
    "        try:\n",
    "            yamldct = (yaml.load(stream, Loader=yaml.FullLoader))\n",
    "        except yaml.YAMLError as e:\n",
    "            print(e)\n",
    "            return 0\n",
    "\n",
    "\n",
    "        # pickOutput's file name and file path will be calculated in the foo_listFilesToRead()\n",
    "\n",
    "\n",
    "    # for data2excel and plotting:\n",
    "    pickleToRead = yamldct[\"filename_inputPickle\"]\n",
    "    yamldct['filepath_inputPickle'] = Path(yamldct['pwd'], 'pickles', f'{pickleToRead}')\n",
    "    yamldct['dirpath_outputs'] = Path(yamldct['pwd'], 'outputs/',)\n",
    "    yamldct['filepath_jpeg'] = Path(yamldct['pwd'], 'outputs/', f'{pickleToRead}.jpg')\n",
    "    yamldct['filepath_excel'] = Path(yamldct['pwd'], 'outputs/', f'{pickleToRead}.xlsx')\n",
    "    yamldct['filepath_csv'] = Path(yamldct['pwd'], 'outputs/', f'{pickleToRead}.csv')\n",
    "\n",
    "\n",
    "    # for raw txt files reading:\n",
    "    rex_partfilePattern = re.compile(\n",
    "        r'''\n",
    "        (?P<subject>.*)\n",
    "        _part(?P<part>\\d+)\n",
    "        \\.txt$\n",
    "        ''',\n",
    "        re.X,\n",
    "    )\n",
    "\n",
    "    rex_replacePartNum = re.compile(\n",
    "        r'''\n",
    "        _part\\d+ \\.txt\n",
    "        ''',\n",
    "        re.X,\n",
    "    )\n",
    "\n",
    "    if yamldct['yes_readEntireFolder']:\n",
    "        folderToRead = yamldct[\"all txts in this folder will be read\"]\n",
    "        yamldct['dirpath_txts'] = Path(yamldct['pwd'], 'raws', f'{folderToRead}')\n",
    "\n",
    "        yamldct['filename_outputPickle'] = folderToRead\n",
    "\n",
    "\n",
    "\n",
    "    else:\n",
    "        yamldct['dirpath_txts'] = Path(yamldct['pwd'], 'raws')\n",
    "\n",
    "        oneofthepartfile = yamldct[\"one of the part files in /raws\"]\n",
    "        match = rex_partfilePattern.match(oneofthepartfile)\n",
    "        yamldct['filename_outputPickle'] = match.group('subject')\n",
    "\n",
    "    yamldct['filepath_outputPickle'] = Path(yamldct['pwd'], 'pickles/', f\"{yamldct['filename_outputPickle']}\")\n",
    "\n",
    "    yamldct['filepath_txt_stub'] = Path(yamldct['dirpath_txts'], yamldct['filename_outputPickle'])\n",
    "    # this is the partial fullpath of a partfile that doesn't have _part# attached\n",
    "    # it is used to generate a list of partfiles\n",
    "    return yamldct\n",
    "\n",
    "\n",
    "def listFilesToRead():\n",
    "    '''\n",
    "    read 0_init.yaml and inferr all paths filenames\n",
    "    it is called by all utils\n",
    "    '''\n",
    "\n",
    "    meta_ = foo_initMeta()\n",
    "\n",
    "    # create folders for rawinput, pickled data, and output\n",
    "    #   if they don't exist yet.\n",
    "    folders = dict()\n",
    "    yes_allNecessaryFoldersExist = 1\n",
    "    for x in ['raws', 'pickles', 'outputs']:\n",
    "        foldername_toBeCreated = x\n",
    "        p = os.path.join(meta_['pwd'], foldername_toBeCreated)\n",
    "        folders[x] = p\n",
    "        if not os.path.exists(p):\n",
    "            os.makedirs(p)\n",
    "            yes_allNecessaryFoldersExist = 0\n",
    "\n",
    "    if not yes_allNecessaryFoldersExist:\n",
    "        import logging\n",
    "        logging.warning(\n",
    "            \"first time run, 'raws', 'pickles', 'outputs' created. Run Again!\"\n",
    "        )\n",
    "        exit(0)\n",
    "\n",
    "\n",
    "    l_return = [] # list of filepaths to return\n",
    "    subject = '' # the pickleoutput filename\n",
    "\n",
    "\n",
    "\n",
    "    if meta_['yes_readEntireFolder']:\n",
    "        # rename files to be partfiles anyways according to modified time\n",
    "\n",
    "        import logging\n",
    "        logging.warning(\n",
    "            'reading entire folder!'\n",
    "        )\n",
    "\n",
    "        dirpath_txts = meta_['dirpath_txts']\n",
    "        subject = meta_['filename_outputPickle']\n",
    "\n",
    "        from collections import OrderedDict\n",
    "        dct_tobeSorted :dict = {}\n",
    "\n",
    "        for x in os.listdir(dirpath_txts):\n",
    "            if '.txt' in x and not(subject+'_part' in x):\n",
    "                xpath = os.path.join(dirpath_txts, x)\n",
    "                if os.path.isfile(xpath) :\n",
    "                    dct_tobeSorted[os.path.getmtime(xpath)] = xpath\n",
    "\n",
    "        dct_sorted = OrderedDict(sorted(dct_tobeSorted.items()))\n",
    "\n",
    "        partnumber = 4\n",
    "        for x in dct_sorted:\n",
    "\n",
    "            newfilepath = os.path.join(dirpath_txts, f'{subject}_part{partnumber:02}.txt')\n",
    "            logging.warning(\n",
    "                f'renamed {dct_sorted[x]} into {newfilepath}'\n",
    "            )\n",
    "            os.rename(\n",
    "                dct_sorted[x],\n",
    "                newfilepath,\n",
    "            )\n",
    "            partnumber += 4\n",
    "\n",
    "    '''\n",
    "    check how many parts file are there to read\n",
    "    parts number does not have to be continuous\n",
    "    '''\n",
    "    filepath_stub = meta_['filepath_txt_stub']\n",
    "\n",
    "    for i in range(0, 100):  # part number must be in the range() here\n",
    "\n",
    "        # we are not using the os.listdir() or os.walk() here\n",
    "        # because we want to make sure that the part files are read\n",
    "        # in the incrementing sequence\n",
    "        replstr = f'_part{i:02}.txt'\n",
    "\n",
    "        xfilepath = filepath_stub.with_name(filepath_stub.name + replstr)\n",
    "        if os.path.isfile(xfilepath):\n",
    "            l_return.append(xfilepath)\n",
    "    meta_['l_filepaths'] = l_return\n",
    "    return meta_\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# [+] This facility read historian txt files, and\n",
    "\n",
    "# [+] munge them into a list of dfs. each df contains one column of data with \n",
    "# column label and index of timestamps\n",
    "# each frame may have different lengths, and each frame will have differently \n",
    "# distributed timestamps. For example:\n",
    "# dataframe0 will have:\n",
    "# index=[8:00, 9:00, 10:00] column0=[1, 2, 3]\n",
    "# dataframe1 will have:\n",
    "# index=[8:23, 8:44, 9:55, 11:00] column0=[2, 4, 1, 11]\n",
    "# (basically the Historian's rawByTime retrieval)\n",
    "\n",
    "# [+] this facility will also infer the subject string either from:\n",
    "#     the datafolder if you choose to read all files\n",
    "#     or _part.txt files if you choose to read part files\n",
    "\n",
    "# [+] then pickle the subject string and list of dfs into data0.p and data.p folder in \\input folder\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:reading entire folder!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading:  data\\raws\\data\\data_part04.txt  ...\n",
      "reading:  data\\raws\\data\\data_part08.txt  ...\n",
      "reading:  data\\raws\\data\\data_part12.txt  ...\n",
      "generating meta file ... data.txt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0: U3.GOV_WGATE_POSN      => length: 2 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1: U3.GOV_PILOTVALVE_SIG  => length: 183 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 2: U3.GOV_SP_IND          => length: 140 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3: U3.GEN_FREQ.VAL        => length: 45 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4: U1.GOV_WGATE_POSN      => length: 194 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 5: U1.GOV_PILOTVALVE_SIG  => length: 315 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 6: U1.GOV_SP_IND          => length: 276 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 7: U1.GEN_FREQ.VAL        => length: 57 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 8: U2.GOV_WGATE_POSN      => length: 2 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "43"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9: U2.GOV_PILOTVALVE_SIG  => length: 447 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10: U2.GOV_SP_IND          => length: 56 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11: U2.GEN_FREQ.VAL        => length: 51 \n",
      "pickling data ... data\\pickles\\data\n"
     ]
    }
   ],
   "source": [
    "\n",
    "######################################################################\n",
    "# [USER Choices]\n",
    "######################################################################\n",
    "\n",
    "yes_filter_fault_data = 0\n",
    "yes_time_constrain = 0\n",
    "histPointsStyle = 'wk'\n",
    "    # LS or KT == 'normal', LM =='lm'\n",
    "######################################################################\n",
    "\n",
    "import pickle\n",
    "meta = listFilesToRead()\n",
    "l_filepathToRead = meta['l_filepaths']\n",
    "subject = meta['filename_outputPickle']\n",
    "filepath_pickle = meta['filepath_outputPickle']\n",
    "\n",
    "\n",
    "l_dfs = []  # list of dfs, contains all individual dfs with their own timestamp\n",
    "\n",
    "for xfilepath in l_filepathToRead:\n",
    "    # build up individual dfs from df_raw\n",
    "    c=r\"\"\"\n",
    "    the format of the raw file (tab separated txt file):\n",
    "    tagname\ttimestamp\tvalue\n",
    "    \\\\KTGS_UCMS\\U12_HDCV_AC_PMP_RUN_AUTO.VALUE\t7/24/2017 09:16:23.875\t1\n",
    "    \\\\KTGS_UCMS\\U12_HDCV_AC_PMP_RUN_AUTO.VALUE\t7/24/2017 09:19:45.703\t0\n",
    "    \\\\KTGS_UCMS\\U12_HDCV_DC_PMP_RUN_AUTO.VALUE\t7/24/2017 09:16:52.859\t1\n",
    "    \\\\KTGS_UCMS\\U12_HDCV_DC_PMP_RUN_AUTO.VALUE\t7/24/2017 09:19:45.703\t0\n",
    "    \\\\KTGS_UCMS\\U12_HDCV_BKUP_TEST.VALUE\t7/24/2017 09:16:22.875\t0\n",
    "    \\\\KTGS_UCMS\\U12_HDCV_BKUP_TEST.VALUE\t7/24/2017 09:22:22.875\t1\n",
    "    \\\\KTGS_UCMS\\U12_HDCV_WTR_LEV_VAL.VALUE\t7/24/2017 09:11:11.171\t12\n",
    "    \\\\KTGS_UCMS\\U12_HDCV_WTR_LEV_VAL.VALUE\t7/24/2017 09:11:15.156\t13\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    print('reading: ', xfilepath, ' ...')\n",
    "    df_raw = pd.read_csv(xfilepath, sep='\\t', thousands=',', parse_dates=[2])  # header=None,\n",
    "    # the kargs 'thousands' is very useful if the number contains ','\n",
    "\n",
    "    df_raw = df_raw.dropna(axis='columns', how='all')  # this is necessary, the first column is empty\n",
    "    # duplicated rows don't seem to matter.\n",
    "    # @ df_raw.columns = ['tagname', 'timestamp', 'value']\n",
    "\n",
    "    uniqueTags = df_raw['tagname'].unique()\n",
    "\n",
    "    list_columnNames = list(df_raw.columns.values)\n",
    "    for i in range(len(uniqueTags)):\n",
    "\n",
    "        # filter out one tag's analogue data\n",
    "        # @idf contains 3 columns 'tagname', 'timestamp', 'value'\n",
    "        idf = df_raw[\n",
    "            df_raw['tagname'] == uniqueTags[i]\n",
    "        ]\n",
    "\n",
    "        \n",
    "        c='''\n",
    "        ######################################################################\n",
    "        [STEP optional]\n",
    "            parsing the historian points format, to extract point name\n",
    "        ######################################################################\n",
    "        '''        \n",
    "        if histPointsStyle=='normal':\n",
    "            # create the new tagname that is rid of fluffs in:\n",
    "            #    '\\\\LSGS_UCMS\\U3.GEN_GDBRG1_TEMP.VAL.VALUE'\n",
    "            #    '\\\\LSGS_UCMS\\U6.STTR_WDG_avgtemp.VALUE'\n",
    "            tempTagname = uniqueTags[i].split('\\\\')[-1]\n",
    "            tempTagname = re.search(r'(?P<keep>[\\w\\.]+?)(?=[_\\.]VAL(UE)?)', tempTagname, re.I).group('keep')\n",
    "            \n",
    "        if histPointsStyle=='wk':        \n",
    "            # WK.U3.GOV_PILOTVALVE_SIG # no val or value at the end\n",
    "            tempTagname = uniqueTags[i][3:]\n",
    "#             tempTagname = re.search(r'(?P<keep>[\\w\\.]+?)(?=[_\\.]VAL(UE)?)', tempTagname, re.I).group('keep')\n",
    "\n",
    "\n",
    "        # change the analogue-data-containing column's column name to be the cim point name\n",
    "        list_columnNames[-1] = tempTagname\n",
    "        idf.columns = list_columnNames\n",
    "        idf = idf.drop('tagname', axis=1)\n",
    "        idf['timestamp'] = pd.to_datetime(idf['timestamp']) # convert into numpy's proprietary datetime64 format\n",
    "\n",
    "        c='''\n",
    "        ######################################################################\n",
    "        [STEP optional]\n",
    "            specify start time and end time, if you want to truncate time\n",
    "        ######################################################################\n",
    "        '''\n",
    "\n",
    "        if yes_time_constrain:\n",
    "\n",
    "            ts_start_ = np.datetime64(datetime.datetime(2017, 2, 16, 3, 30, 0))\n",
    "            ts_end_ = np.datetime64(datetime.datetime(2017, 2, 17, 0, 0, 0))\n",
    "\n",
    "            idf = idf[\n",
    "                (idf['timestamp'] > ts_start_) & (idf['timestamp'] < ts_end_)\n",
    "            ]\n",
    "\n",
    "\n",
    "\n",
    "        c='''\n",
    "        ######################################################################\n",
    "        [STEP optional]\n",
    "            filtering out fault data like int being 32767, in ucms, that means\n",
    "            fault\n",
    "        ######################################################################\n",
    "        '''\n",
    "        if yes_filter_fault_data:\n",
    "            idf.loc[\n",
    "                (idf[tempTagname] > 200) | (idf[tempTagname] < -200),\n",
    "                tempTagname\n",
    "            ] = np.nan\n",
    "\n",
    "        idf = idf.drop_duplicates(subset='timestamp', keep='first')\n",
    "        # When joining dfs side by side (instead of joining dfs on top of each other)\n",
    "        #   we can not have duplicated index.\n",
    "        # [?] there are duplicated timestemp within an idf? anyways, this shouldn't cause any harm\n",
    "        # [todo] investigate this syntax:\n",
    "        #\tdf3 = df3[~df3.index.duplicated(keep='first')]\n",
    "        #\tthis solves the problem of dropping index, while the index column doesn't have a name\n",
    "        # [todo] concat by join='inner' does NOT get rid of repeated index, unless the entire row is the same\n",
    "\n",
    "        idf = idf.set_index(['timestamp'])\n",
    "        # @ idf ==> index=timestamp column0=U3.GEN_CUR_C\n",
    "\n",
    "        flag0 = 0\n",
    "        for i, xdf in enumerate(l_dfs):\n",
    "            if idf.columns == xdf.columns:\n",
    "                print('found existing columns')\n",
    "                l_dfs[i] = pd.concat([xdf, idf], join = 'outer', axis=0)\n",
    "                flag0 = 1\n",
    "\n",
    "        if not flag0:\n",
    "            l_dfs.append(idf)\n",
    "\n",
    "print(f'generating meta file ... {meta[\"filename_outputPickle\"]}'+'.txt')\n",
    "with open(\n",
    "        str(meta['filepath_outputPickle']) + '.txt',\n",
    "        'w',\n",
    ") as output_file:\n",
    "    for idx, item in enumerate(l_dfs):\n",
    "        tstr = '{:>2}: {:<22} => length: {} \\n'.format(idx, item.columns[0], item.shape[0])\n",
    "        output_file.write(tstr)\n",
    "        print(tstr, end='')\n",
    "\n",
    "print(f\"pickling data ... {meta['filepath_outputPickle']}\")\n",
    "with open(\n",
    "    meta['filepath_outputPickle'],\n",
    "    'wb',\n",
    ") as output_file:\n",
    "    pickle.dump(l_dfs, output_file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    # VERY IMPORTANT to have the 'protocol' keyword argument, otherwise this won't work\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\njoin list of dfs together, and use xlwings to export them\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading pickle file ... \n",
      " ==> data\\pickles\\data\n",
      "saving excel file ... \n",
      " ==> data\\outputs\\data.xlsx\n",
      "reformatting... <xlsxwriter.workbook.Workbook object at 0x000002B827712B20>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import pandas as pd\n",
    "import xlsxwriter\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# [USER Choices]\n",
    "######################################################################\n",
    "yes_fillnan = 0\n",
    "yes_outputXlwings = 0\n",
    "yes_outputExcel = 1\n",
    "yes_outputCSV = 0\n",
    "######################################################################\n",
    "\n",
    "\n",
    "'''\n",
    "join list of dfs together, and use xlwings to export them\n",
    "'''\n",
    "\n",
    "yaml_ = foo_initMeta()\n",
    "\n",
    "filepath = yaml_['filepath_inputPickle']\n",
    "\n",
    "print(f'reading pickle file ... \\n ==> {filepath}')\n",
    "with open(\n",
    "        filepath,\n",
    "        'rb',\n",
    ") as input_file:\n",
    "    l_dfs = pickle.load(input_file)\n",
    "\n",
    "# with open(r'input/data0', 'rb') as input_file:\n",
    "#     dict_aux = pickle.load(input_file)\n",
    "#\n",
    "# if not dict_aux:\n",
    "#     dict_aux = {'subject': 'result'}\n",
    "\n",
    "df0 = pd.concat(l_dfs, axis=1, join='outer')\n",
    "    # amazingly pandas' outer join can take care of unalinged timestamp merging\n",
    "    # as long as timestamp is the index.\n",
    "\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# [USER Choice] fill nan\n",
    "######################################################################\n",
    "\n",
    "\n",
    "if yes_fillnan:\n",
    "    df0 = df0.fillna(\n",
    "        method='ffill', # forward fill, meaning, fill with Previous data\n",
    "        axis=0, # doc says \"index\", but it really means \"along the column\"\n",
    "    )\n",
    "    df0 = df0.fillna(\n",
    "        method='bfill', # what if the first data is NaN? well fill backward\n",
    "        axis=0,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# [USER Choice] xlwings output\n",
    "######################################################################\n",
    "\n",
    "if yes_outputXlwings:\n",
    "    import importlib.util\n",
    "    spec = importlib.util.spec_from_file_location('asdf', r'C:\\__PYP\\__COMMON\\foo_df2xw.py')\n",
    "    myModuleName0_foo_df2xw = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(myModuleName0_foo_df2xw)\n",
    "\n",
    "    xwsheet = myModuleName0_foo_df2xw.foo_df2xw(df0)\n",
    "\n",
    "    xwsheet.range('A:A').number_format = 'yyyy-mm-dd hh:mm:ss'\n",
    "    xwsheet.range('B:ZZ').number_format = '00.00'\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# [USER Choice] xlsxwriter output\n",
    "######################################################################\n",
    "\n",
    "if yes_outputExcel:\n",
    "    filepath_excel = yaml_['filepath_excel']\n",
    "    # writer = pd.ExcelWriter(filepath_excel, engine='xlsxwriter')\n",
    "\n",
    "    #\n",
    "    # workbook = xlsxwriter.Workbook(filepath_excel, {'default_date_format':\n",
    "    #                                                   'dd/mm/yy'})\n",
    "\n",
    "    print(f'saving excel file ... \\n ==> {filepath_excel}')\n",
    "\n",
    "    writer = pd.ExcelWriter(\n",
    "        filepath_excel,\n",
    "        engine='xlsxwriter',\n",
    "    )\n",
    "\n",
    "    # [v] very necessary step to disabble pd's header format\n",
    "    #    for some reason, this header format is applied at the end of xlsxwriter.save() operation\n",
    "    #    so any attempts to apply header format yourself is futile\n",
    "\n",
    "    try:\n",
    "        import pandas.io.formats.excel\n",
    "        pandas.io.formats.excel.header_style = None\n",
    "    except Exception as e:\n",
    "        print('attempting disable pandas default header format ==>', e)\n",
    "\n",
    "    df0.to_excel(\n",
    "        excel_writer=writer,\n",
    "        index=True,\n",
    "        sheet_name='raw',\n",
    "        na_rep=\"--\"\n",
    "    )\n",
    "\n",
    "    workbook :xlsxwriter.Workbook = writer.book\n",
    "    print('reformatting...', workbook)\n",
    "    worksheet = writer.sheets['raw']\n",
    "\n",
    "    cellformat0 = workbook.add_format(\n",
    "        {\n",
    "            'num_format':'0.00',\n",
    "            'font_size':12,\n",
    "            'align':'right',\n",
    "            'shrink':True,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    worksheet.set_column(0, 0, 20) # column 0 to 0, width of 20\n",
    "    # [!!!] It is IMPOSSIBLE to format date/datetime column data using set_column()\n",
    "    # it is impossible to format the header row.\n",
    "    # xlsx writer is powerless to format anything preformatted by pandas\n",
    "    # xlsx can still adjust width and height of columns and rows\n",
    "    worksheet.set_column(1, 33, 8, cellformat0)\n",
    "\n",
    "   #--------------------------------------------------------------------------\n",
    "    # [v] format header rows\n",
    "\n",
    "    comment='''\n",
    "    # [philosophical] can not do accumulative formatting in xlsxwriter, that's why\n",
    "    after column formatting, header format needs to be changed\n",
    "\n",
    "    unlike in excel, you can apply accumulative formatting to a cell, ie. apply\n",
    "    row format bold and column format fontcolor overlaps at a cell. that cell will \n",
    "    be bold and fontcolor. xlsxwriter can not do that.\n",
    "\n",
    "    mostly because every combo of format you want must have an underlying xlsxwriter\n",
    "    format object. Excel does this in the background so accumulative formatting\n",
    "    is possible.\n",
    "    '''\n",
    "\n",
    "    # # [!!!] according to xlsxWriter doc: Pandas writes the dataframe header with a hardcoded cell\n",
    "    # # format. Since it is a cell format it cannot be overridden using set_row().\n",
    "\n",
    "    headerformatdict = {'align':'left', 'valign':'bottom', 'rotation':75, 'right': True, 'bold':True}\n",
    "    headerformat = workbook.add_format(headerformatdict)\n",
    "    for i, headername in enumerate(df0.columns.values):\n",
    "        successful = worksheet.write(0,i+1, headername, headerformat)\n",
    "    successful = worksheet.freeze_panes(1, 1)\n",
    "\n",
    "    writer.save()\n",
    "\n",
    "\n",
    "######################################################################\n",
    "# [USER Choice] xlsxwriter or csv output\n",
    "######################################################################\n",
    "\n",
    "if yes_outputCSV:\n",
    "    df0.to_csv(yaml_['filepath_csv'], sep='\\t', encoding='utf-8')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
